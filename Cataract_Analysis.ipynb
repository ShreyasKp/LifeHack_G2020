{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595749029996",
   "display_name": "Python 3.6.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LifeHack 2020- Problem #2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vision Impairment can be a serious implication on one’s health. According to the key facts stated by the World Health Organization “Globally, at least 2.2 billion people have a vision impairment or blindness, of whom at least 1 billion have a vision impairment that could have been prevented or has yet to be addressed.” Do you have the ability to better detect potential vision issues and help change thousands, if not, millions of lives? \n",
    "\n",
    "For example, the given dataset looks at cataract and normal eye image dataset for cataract detection. Participants are encouraged to use other available datasets to better analyse and share their findings regarding the detection of the potential risk of visual problems detection, preferably relating to the Asia region. Indeed this problem statement covers a wide area of visual impairments and does not need to focus solely on cataract. Participants are free to explore other options of eye disease detection.  \n",
    "\n",
    "  • Present Insight into the data by mean of visualization and/or interactive means\n",
    "  • Prediction model for eye disease \n",
    "\n",
    "For reference:\n",
    "https://www.kaggle.com/jr2ngb/cataractdataset?Vision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import matplotlib                  # 2D Plotting Library\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns              # Python Data Visualization Library based on matplotlib\n",
    "#import geopandas as gpd            # Python Geospatial Data Library\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "# Importing all necessary libraries \n",
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "from keras.models import Sequential \n",
    "from keras.layers import Conv2D, MaxPooling2D \n",
    "from keras.layers import Activation, Dropout, Flatten, Dense \n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 2464, 1632"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Train / Val / Test folders \n",
    "root_dir = 'cataract_dataset_bundle_archive'\n",
    "normal = '/1_normal'\n",
    "cataract = '/2_cataract'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(root_dir +'/train' + normal)\n",
    "os.makedirs(root_dir +'/train' + cataract)\n",
    "os.makedirs(root_dir +'/val' + normal)\n",
    "os.makedirs(root_dir +'/val' + cataract)\n",
    "os.makedirs(root_dir +'/test' + normal)\n",
    "os.makedirs(root_dir +'/test' + cataract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "current = normal\n",
    "src = root_dir + current # Folder to copy images from\n",
    "\n",
    "allFiles = os.listdir(src)\n",
    "np.random.shuffle(allFiles)\n",
    "train_Files, val_Files, test_Files = np.split(np.array(allFiles),\n",
    "                                                          [int(len(allFiles)*0.7), int(len(allFiles)*0.85)])\n",
    "\n",
    "\n",
    "train_Files = [src+'/'+ name for name in train_Files.tolist()]\n",
    "val_Files = [src+'/' + name for name in val_Files.tolist()]\n",
    "test_Files = [src+'/' + name for name in test_Files.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total images:  300\nTraining:  210\nValidation:  45\nTesting:  45\n"
    }
   ],
   "source": [
    "print('Total images: ', len(allFiles))\n",
    "print('Training: ', len(train_Files))\n",
    "print('Validation: ', len(val_Files))\n",
    "print('Testing: ', len(test_Files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in train_Files:\n",
    "    shutil.copy(name, root_dir + \"/train\" + current)\n",
    "\n",
    "for name in val_Files:\n",
    "    shutil.copy(name, root_dir + \"/val\" + current)\n",
    "\n",
    "for name in test_Files:\n",
    "    shutil.copy(name, root_dir + \"/test\" + current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}